{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CS 289A Homework 5 - Data Processing\n",
    "------------------------------------------------\n",
    "This script will load the data sets in need of preprocessing (census and Titanic) and perform the preprocessing for better learning results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with the overhead: load necessary modules and trigger them to reload when they are modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn.feature_extraction import DictVectorizer as DV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, specify paths to data on the local machine. \n",
    "\n",
    "**You must change the path to fit your data. Also note that the distributed census and Titanic training data/test csv files have been renamed.**  \n",
    "(among other changes, the filename now includes the suffix _raw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = \"/Users/mitch/Documents/Cal/2_2017_Spring/COMPSCI 289A - Intro to Machine Learning/HW05/\"\n",
    "\n",
    "CENS_RAWPATH = \"Data/hw5_census_dist/census_traindata_raw.csv\"\n",
    "CENS_TSTPATH = \"Data/hw5_census_dist/census_testdata_raw.csv\"\n",
    "CENS_CLNTRNPATH = \"Data/census_traindata.csv\"\n",
    "CENS_LBLTRNPATH = \"Data/census_traindata_lbl.csv\"\n",
    "CENS_VECTRNPATH = \"Data/census_traindata_vec.csv\"\n",
    "CENS_CLNTSTPATH = \"Data/census_testdata.csv\"\n",
    "CENS_VECTSTPATH = \"Data/census_testdata_vec.csv\"\n",
    "\n",
    "TITA_RAWPATH = \"Data/hw5_titanic_dist/titanic_traindata_raw.csv\"\n",
    "TITA_TSTPATH = \"Data/hw5_titanic_dist/titanic_testdata_raw.csv\"\n",
    "TITA_CLNTRNPATH = \"Data/titanic_traindata.csv\"\n",
    "TITA_LBLTRNPATH = \"Data/titanic_traindata_lbl.csv\"\n",
    "TITA_VECTRNPATH = \"Data/titanic_traindata_vec.csv\"\n",
    "TITA_CLNTSTPATH = \"Data/titanic_testdata.csv\"\n",
    "TITA_VECTSTPATH = \"Data/titanic_testdata_vec.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute and clean the datasets\n",
    "----------------------------------------\n",
    "First we will impute and clean the census data: (According to the census data README, it appears that the fnlwgt feature denotes similarity of individuals in a state. The census data is not necessarily state separated, so remove that feature.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "censusdf_rawtrain = pandas.read_csv(open(BASE_DIR+CENS_RAWPATH))\n",
    "censusdf_rawtest = pandas.read_csv(open(BASE_DIR+CENS_TSTPATH))\n",
    "\n",
    "censusdf_nantrain = censusdf_rawtrain.replace(to_replace='?',value=np.nan)\n",
    "censusdf_nantest = censusdf_rawtest.replace(to_replace='?',value=np.nan)\n",
    "\n",
    "censusdf_train = censusdf_nantrain.fillna(censusdf_nantrain.mode().iloc[0])\n",
    "censusdf_test = censusdf_nantest.fillna(censusdf_nantest.mode().iloc[0])\n",
    "\n",
    "censusdf_train.drop('fnlwgt',axis=1,inplace=True)\n",
    "censusdf_test.drop('fnlwgt',axis=1,inplace=True)\n",
    "\n",
    "censusdf_train.to_csv(BASE_DIR+CENS_CLNTRNPATH,index=False)\n",
    "censusdf_test.to_csv(BASE_DIR+CENS_CLNTSTPATH,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the vast majority of the unknown values, denoted with a '?', are categorical rather than continuous datapoints, replace them with the most common category-value in that feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, repeat the imputation and cleaning for the Titanic dataset: (Recognizing that the cabin feature vector is incredibly sparse--and presumably meaningless--eliminate it from the data set to be processed; similarly, due to the large variation in data types in the ticket column, remove it as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanicdf_rawtrain = pandas.read_csv(open(BASE_DIR+TITA_RAWPATH))\n",
    "titanicdf_rawtest = pandas.read_csv(open(BASE_DIR+TITA_TSTPATH))\n",
    "\n",
    "titanicdf_nantrain = titanicdf_rawtrain.replace(to_replace='',value=np.nan)\n",
    "titanicdf_nantest = titanicdf_rawtest.replace(to_replace='',value=np.nan)\n",
    "\n",
    "titanicdf_train = titanicdf_nantrain.fillna(titanicdf_nantrain.mode().iloc[0])\n",
    "titanicdf_test = titanicdf_nantest.fillna(titanicdf_nantest.mode().iloc[0])\n",
    "\n",
    "\n",
    "titanicdf_train.drop('cabin',axis=1,inplace=True)\n",
    "titanicdf_train.drop('ticket',axis=1,inplace=True)\n",
    "titanicdf_test.drop('cabin',axis=1,inplace=True)\n",
    "titanicdf_test.drop('ticket',axis=1,inplace=True)\n",
    "\n",
    "titanicdf_train.to_csv(BASE_DIR+'/'+TITA_CLNTRNPATH,index=False)\n",
    "titanicdf_test.to_csv(BASE_DIR+'/'+TITA_CLNTSTPATH,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the cleaned and full data\n",
    "--------------------------------------\n",
    "First, separate the labels from the data, and save to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "censuslbldf = censusdf_train['label']\n",
    "censuslbldf.to_csv(BASE_DIR+'/'+CENS_LBLTRNPATH,index=False)\n",
    "censusdf_train.drop('label',axis=1,inplace=True)\n",
    "\n",
    "titaniclbldf = titanicdf_train['survived']\n",
    "titaniclbldf.to_csv(BASE_DIR+'/'+TITA_LBLTRNPATH,index=False)\n",
    "titanicdf_train.drop('survived',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the DictVectorizer class from sklearn to create vectors for categorical mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For the census dataset\n",
    "censusdict_train = censusdf_train.to_dict('records')\n",
    "censusdict_test = censusdf_test.to_dict('records')\n",
    "\n",
    "dv = DV(sparse=False)\n",
    "\n",
    "censusvec_train = dv.fit_transform(censusdict_train)\n",
    "censusvec_test = dv.fit_transform(censusdict_test)\n",
    "\n",
    "np.savetxt(BASE_DIR+CENS_VECTRNPATH,censusvec_train,fmt='%10d',delimiter=',')\n",
    "np.savetxt(BASE_DIR+CENS_VECTSTPATH,censusvec_test,fmt='%10d',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For the Titanic dataset\n",
    "titanicdict_train = titanicdf_train.to_dict('records')\n",
    "titanicdict_test = titanicdf_test.to_dict('records')\n",
    "\n",
    "dv = DV(sparse=False)\n",
    "\n",
    "titanicvec_train = dv.fit_transform(titanicdict_train)\n",
    "titanicvec_test = dv.fit_transform(titanicdict_test)\n",
    "\n",
    "np.savetxt(BASE_DIR+TITA_VECTRNPATH,titanicvec_train,fmt='%10d',delimiter=',')\n",
    "np.savetxt(BASE_DIR+TITA_VECTSTPATH,titanicvec_test,fmt='%10d',delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
