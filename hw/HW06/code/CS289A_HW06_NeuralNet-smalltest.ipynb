{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Constructor\n",
    "\n",
    "This notebook works through the construction of a two-layer neural network. (It will eventually be imported into its own python module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import HW06_utils as ut\n",
    "\n",
    "import gradients as grad\n",
    "import activationfns as af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNet:\n",
    "    \"\"\"\n",
    "    Train and store a neural network, based on supplied training data. \n",
    "    Use this network to predict classifications.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,nlayers=3,unitsperlayer=None,actfns=[af.sigmoid,af.sigmoid],Gradients=None,verbose=False):\n",
    "        \"\"\"\n",
    "        Initialize the neural network\n",
    "        - nlayers:       the number of layers in the neural network (includes input and output layers)\n",
    "        - unitsperlayer: a list specifying (in order) the number of units in all sequential layers except input\n",
    "        - actfns:        a list specifying (in order) the activation function used by all sequential layers except input\n",
    "        - Gradient:      a class providing optimized gradient calculations for the given sequence of  \n",
    "                         activation functions\n",
    "        - verbose:       a boolean for descriptive output\n",
    "        \"\"\"\n",
    "        if unitsperlayer == None:\n",
    "            unitsperlayer = 3*np.ones(nlayers)\n",
    "        elif nlayers == len(unitsperlayer)+1:    \n",
    "            self.nlayers = nlayers-1\n",
    "            self.unitsperlayer = unitsperlayer \n",
    "        elif nlayers > len(unitsperlayer)+1:\n",
    "            print('ERROR: The number of units per layer were not given for at least one layer.')\n",
    "        elif nlayers < len(unitsperlayer)+1:\n",
    "            print('ERROR: More layers were given units than were specified by input \"nlayers\".')\n",
    "        if nlayers == len(actfns)+1:\n",
    "            self.actfns = actfns\n",
    "        elif nlayers > len(actfns)+1:\n",
    "            print('ERROR: The activation function was not given for at least one layer.')\n",
    "        elif nlayers < len(actfns)+1:\n",
    "            print('ERROR: More activation functions were provided than specified by input \"nlayers\".')\n",
    "        if Gradients == None:\n",
    "            print('ERROR: A gradient generator class must be included.')\n",
    "        self.gradients = Gradients\n",
    "        self.weight_matrices = []\n",
    "        \n",
    "    \n",
    "    def initialize_weights(self,shape,mu=0,var=1):\n",
    "        \"\"\"\n",
    "        Initialize weight matrix from normal distribution.\n",
    "        - shape: tuple specifying desired shape of weight matrix\n",
    "        - mu:    mean value of normal distribution\n",
    "        - var:   variance of normal distribution\n",
    "        \"\"\"\n",
    "        weight_matrix = np.random.normal(loc=mu,scale=np.sqrt(var),size=shape)\n",
    "        \n",
    "        return weight_matrix\n",
    "    \n",
    "    \n",
    "    def weight_matrix_shape(self,n):\n",
    "        \"\"\"\n",
    "        Create weight matrix with the proper number of rows and columns for this layer\n",
    "        - n: the layer which will employ an activation function on the product of the \n",
    "             weight matrix and values\n",
    "        \"\"\"\n",
    "        if n != 0 and n != range(self.nlayers)[-1]:\n",
    "            WM_nrows = self.unitsperlayer[n]-1\n",
    "            WM_ncols = self.unitsperlayer[n-1]\n",
    "        elif n == 0:\n",
    "            WM_nrows = self.unitsperlayer[n]-1\n",
    "            WM_ncols = len(data[0])+1\n",
    "        else:\n",
    "            WM_nrows = self.unitsperlayer[n]\n",
    "            WM_ncols = self.unitsperlayer[n-1]\n",
    "        return WM_nrows,WM_ncols\n",
    "    \n",
    "    \n",
    "    def forward(self,data):\n",
    "        \"\"\"\n",
    "        Perform forward pass through neural network by multiplying data by weights\n",
    "        and enforcing a nonlinear activation function for each layer.\n",
    "        - data:           Nxd numpy array with N sample points and d features\n",
    "        - weightmatrices: ordered list of sequential weight matrices corresponding to layers\n",
    "        - actfns:         ordered list of sequential activation functions corresponding to layers\n",
    "                         (functions are defined in activationfuncs.py)\n",
    "        Returns layeroutputs, a list of the outputs from each layer. The last entry\n",
    "        is an CxN numpy array with hypotheses for each sample N_i being in class C_j.\n",
    "        \"\"\"\n",
    "        H = data.T\n",
    "        layeroutputs = []\n",
    "        for i in range(self.nlayers):\n",
    "            W = self.weight_matrices[i]\n",
    "            actfn = self.actfns[i]\n",
    "            H = actfn(np.dot(W,H))\n",
    "            # If the layer is not the output layer, add a fictitious unit for bias terms\n",
    "            if i != self.nlayers-1:\n",
    "                fictu = np.array([np.ones_like(H[0])])\n",
    "                H = np.concatenate((H,fictu),axis=0)\n",
    "            layeroutputs.append(H)\n",
    "        return layeroutputs\n",
    "    \n",
    "    \n",
    "    def backward(self,layeroutputs,labelrange,gradients=None):\n",
    "        \"\"\"\n",
    "        Perform backward pass through neural network by computing gradients of \n",
    "        input weight matrices with respect to the loss function comparing hypotheses \n",
    "        to true values. Classes for gradients are provided in gradients.py module \n",
    "        (a unique gradient class is required for neural networks with different \n",
    "        numbers of layers and/or different activation functions)\n",
    "        \"\"\"\n",
    "        if gradients == None:\n",
    "            Gradients = self.gradients\n",
    "        gradients = Gradients.calculate(self.weight_matrices,layeroutputs,labelrange)\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    \n",
    "    def classify_outputs(self,finaloutputs):\n",
    "        \"\"\"\n",
    "        Convert final outputs into classifications\n",
    "        -finaloutputs: a CxN numpy array with hypotheses for each sample N_i being in\n",
    "                       class C_j.\n",
    "        Returns a 1D, length-N array with values corresponding to point classifications\n",
    "        \"\"\"\n",
    "        if len(finaloutputs) == 1:\n",
    "            classifications = np.around(finaloutputs[0]).astype(int)\n",
    "        if len(finaloutputs) > 1:\n",
    "            # Add one for 1-indexing in classification labels\n",
    "            classifications = (np.argmax(finaloutputs,axis=0)+np.ones(len(finaloutputs[0]))).astype(int)\n",
    "        return classifications\n",
    "    \n",
    "    \n",
    "    def stoch_grad_descent_prep(self,layeroutputs,classifications,labels):\n",
    "        \"\"\"\n",
    "        For stochastic gradient descent, choose one misclassified point from the data set\n",
    "        (index i) for performing backprop algorithm and reduce datasets accordingly     \n",
    "        - layeroutputs: a list of the outputs from each layer\n",
    "        - predictions:  1D, length-N numpy array with predictions for the N sample points\n",
    "        - labels:       1D, length-N numpy array with true labels for the N sample points\n",
    "        \"\"\"\n",
    "        diffclass = True\n",
    "        tested_i = []\n",
    "        while diffclass:\n",
    "            i = np.random.randint(len(labels))\n",
    "            if i not in tested_i:\n",
    "                tested_i.append(i)\n",
    "            if len(tested_i) == len(labels):\n",
    "                print('All points classified correctly')\n",
    "                return True,True\n",
    "            if labels[i]!=classifications[i]:\n",
    "                # Improperly classified point, so use it for gradient descent\n",
    "                layeroutputs_i = [layeroutput[:,i] for layeroutput in layeroutputs]\n",
    "                return layeroutputs_i,i\n",
    "    \n",
    "    \n",
    "    def train(self,data,labels,epsilon=0.01):\n",
    "        \"\"\"\n",
    "        Train the neural network on input data\n",
    "        - data:   Nxd numppy array with N sample points and d features\n",
    "        - labels: 1D, length-N numpy array with labels for the N sample points\n",
    "        \"\"\"\n",
    "        # Ensure labels are integers\n",
    "        labels = labels.astype(int)\n",
    "       \n",
    "        # Add fictitious unit for bias terms\n",
    "        fictu = np.array([np.ones(len(data))]).T\n",
    "        data = np.concatenate((data,fictu),axis=1)\n",
    "    \n",
    "        # Initialize Weights\n",
    "        for n in range(self.nlayers):\n",
    "            WM_nrows,WM_ncols = self.weight_matrix_shape(n)\n",
    "            # Variance of weight matrix determined by fan-in (eta), the number of units in the previous layer \n",
    "            # (or the number of data features when initializing the first weight matrix)\n",
    "            eta = WM_ncols\n",
    "            weight_matrix = self.initialize_weights((WM_nrows,WM_ncols),mu=0,var=(1/eta))\n",
    "            print(weight_matrix)\n",
    "            self.weight_matrices.append(weight_matrix)\n",
    "        \n",
    "        # Execute gradient class overhead before beginning training loop\n",
    "        self.gradients.prepare(data,labels,self.unitsperlayer[-1])\n",
    "        \n",
    "        # Begin loop\n",
    "        layeroutputs = self.forward(data)\n",
    "        classifications = self.classify_outputs(layeroutputs[-1])\n",
    "        trainAccs = [ut.score_accuracy(classifications,labels)]\n",
    "        counter=0\n",
    "        while counter < 1000:\n",
    "            layeroutputs_i,label_i = self.stoch_grad_descent_prep(layeroutputs,classifications,labels)\n",
    "            if layeroutputs_i == True:\n",
    "                break\n",
    "            gradients = self.backward(layeroutputs_i,[label_i,label_i+1])\n",
    "\n",
    "            for n in range(self.nlayers):\n",
    "                self.weight_matrices[n]=self.weight_matrices[n]-epsilon*gradients[n]\n",
    "            layeroutputs = self.forward(data)\n",
    "            \n",
    "            classifications = self.classify_outputs(layeroutputs[-1])\n",
    "            if counter%1 == 0:\n",
    "                print(counter)\n",
    "                print(layeroutputs[-1])\n",
    "                print('rounded',classifications)\n",
    "            trainAccs.append(ut.score_accuracy(classifications,labels))\n",
    "            counter+=1\n",
    "        print(trainAccs)\n",
    "        \n",
    "        \n",
    "    def predict(self,testdata):\n",
    "        \"\"\"\n",
    "        Predict classfications for unlabeled data points using the previously \n",
    "        trained neural network.\n",
    "        - testdata: Nxd numpy array with N sample points and d features\n",
    "                    *Note, dimension d must match that used for the data array in NeuralNet.train*\n",
    "        Returns a 1D, length-N numpy array of predictions (one prediction per point)\n",
    "        \"\"\"\n",
    "        npoints = len(testdata)\n",
    "        predictions = np.empty(npoints)\n",
    "        layeroutputs = self.forward(data)\n",
    "        predictions = classify_outputs(layeroutputs[-1])\n",
    "\n",
    "        return predictions.astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.array([[2,2,2,5],[1,1,1,2],[10,3,1,6],[2,2,2,5],[2,1,5,3],[2,2,2,6],[8,1,2,1]])\n",
    "data = data-np.mean(data,axis=0)\n",
    "labels = np.array([1,1,2,1,1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = NeuralNet(nlayers=3,unitsperlayer=[3,2],actfns=[af.tanh,af.sigmoid],Gradients=grad.tanhsig2layer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.6287949   0.2859587   0.42179369  0.3511788   0.2810423 ]\n",
      " [-0.55797678 -0.19553882  0.24463676 -0.14781208  0.31511919]]\n",
      "[[-1.53104875  0.25296366  0.73007487]\n",
      " [ 0.17738619 -0.17652865 -0.65354257]]\n",
      "0\n",
      "[[ 0.3745752   0.51620623  0.87816082  0.3745752   0.37759347  0.36210644\n",
      "   0.88151179]\n",
      " [ 0.34701742  0.3270297   0.34648701  0.34701742  0.34066926  0.35036845\n",
      "   0.34221693]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "1\n",
      "[[ 0.37616265  0.51815094  0.87531385  0.37616265  0.379599    0.36358251\n",
      "   0.87883157]\n",
      " [ 0.3459014   0.32615363  0.35092501  0.3459014   0.33927613  0.34929722\n",
      "   0.34648498]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "2\n",
      "[[ 0.37774738  0.52008633  0.87241952  0.37774738  0.38160191  0.36505624\n",
      "   0.87610819]\n",
      " [ 0.34479495  0.3252893   0.3553583   0.34479495  0.33789564  0.34823491\n",
      "   0.35074861]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "3\n",
      "[[ 0.37931243  0.5211523   0.86957083  0.37931243  0.38337263  0.36664522\n",
      "   0.87329761]\n",
      " [ 0.34369639  0.32461779  0.35967215  0.34369639  0.33665583  0.3470992\n",
      "   0.35507879]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "4\n",
      "[[ 0.38088665  0.5222223   0.86667392  0.38088665  0.38514848  0.36824614\n",
      "   0.87043736]\n",
      " [ 0.34259802  0.32394634  0.36398533  0.34259802  0.33542111  0.34596175\n",
      "   0.35941165]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "5\n",
      "[[ 0.38247188  0.52411744  0.86364102  0.38247188  0.38714142  0.36972845\n",
      "   0.8675753 ]\n",
      " [ 0.34151261  0.32311229  0.36839869  0.34151261  0.33407654  0.34491451\n",
      "   0.36366899]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "6\n",
      "[[ 0.38406112  0.52518364  0.86064976  0.38406112  0.38891948  0.37135304\n",
      "   0.8646143 ]\n",
      " [ 0.34041721  0.32244874  0.37270264  0.34041721  0.33285775  0.34377403\n",
      "   0.36800604]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "7\n",
      "[[ 0.38565782  0.52625299  0.85761056  0.38565782  0.39070159  0.37298729\n",
      "   0.86160432]\n",
      " [ 0.33932322  0.32178576  0.37700245  0.33932322  0.33164458  0.34263359\n",
      "   0.372341  ]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "8\n",
      "[[ 0.38726132  0.52732498  0.85452364  0.38726132  0.39248721  0.37463041\n",
      "   0.85854564]\n",
      " [ 0.33823109  0.32112365  0.38129687  0.33823109  0.33043731  0.34149378\n",
      "   0.37667241]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "9\n",
      "[[ 0.38884634  0.5291637   0.85130829  0.38884634  0.39446408  0.37612405\n",
      "   0.85549773]\n",
      " [ 0.33717304  0.32032805  0.3856697   0.33717304  0.32913973  0.34046604\n",
      "   0.38090854]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "10\n",
      "[[ 0.39046127  0.53023031  0.84812841  0.39046127  0.39624954  0.37778607\n",
      "   0.85234026]\n",
      " [ 0.33608612  0.31967449  0.38994635  0.33608612  0.3279491   0.33932661\n",
      "   0.38523299]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "11\n",
      "[[ 0.39208133  0.5312987   0.84490159  0.39208133  0.39803739  0.3794548\n",
      "   0.84913523]\n",
      " [ 0.33500215  0.31902225  0.39421438  0.33500215  0.32676482  0.33818935\n",
      "   0.3895496 ]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "12\n",
      "[[ 0.39366311  0.53309569  0.84155178  0.39366311  0.40000033  0.38095273\n",
      "   0.84595101]\n",
      " [ 0.33396577  0.3182551   0.39854694  0.33396577  0.3255021   0.33717843\n",
      "   0.39375781]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "13\n",
      "[[ 0.3952921   0.53415767  0.83823387  0.3952921   0.40178637  0.38263706\n",
      "   0.8426494 ]\n",
      " [ 0.33288839  0.31761173  0.40279135  0.33288839  0.32433479  0.3360437\n",
      "   0.39805992]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "14\n",
      "[[ 0.39692464  0.53522058  0.83487019  0.39692464  0.40357372  0.38432607\n",
      "   0.83930169]\n",
      " [ 0.33181489  0.3169701   0.40702413  0.33181489  0.32317421  0.3349125\n",
      "   0.40235033]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "15\n",
      "[[ 0.39850152  0.53697533  0.83138944  0.39850152  0.40552102  0.38582637\n",
      "   0.835984  ]\n",
      " [ 0.33080031  0.31623054  0.41130874  0.33080031  0.32194577  0.33391895\n",
      "   0.40652229]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "16\n",
      "[[ 0.40007365  0.53871849  0.82786536  0.40007365  0.40746287  0.38732244\n",
      "   0.83262567]\n",
      " [ 0.3297949   0.31550141  0.41557548  0.3297949   0.32072902  0.33293417\n",
      "   0.41067745]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "17\n",
      "[[ 0.40171608  0.53976609  0.82437054  0.40171608  0.4092423   0.38903215\n",
      "   0.82913659]\n",
      " [ 0.32873284  0.31487599  0.41976215  0.32873284  0.31959586  0.33180797\n",
      "   0.41493713]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "18\n",
      "[[ 0.40328247  0.54148192  0.82076253  0.40328247  0.41117248  0.39052632\n",
      "   0.82569381]\n",
      " [ 0.32774278  0.3141653   0.42399137  0.32774278  0.31840165  0.33083629\n",
      "   0.41906103]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "19\n",
      "[[ 0.40492939  0.5425216   0.81718249  0.40492939  0.41294722  0.39224575\n",
      "   0.82211455]\n",
      " [ 0.32668938  0.31354893  0.42814435  0.32668938  0.31728556  0.32971602\n",
      "   0.42329316]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "20\n",
      "[[ 0.4064894   0.54420985  0.81349314  0.4064894   0.414865    0.39373732\n",
      "   0.81858951]\n",
      " [ 0.32571458  0.31285628  0.43233328  0.32571458  0.31611357  0.32875747\n",
      "   0.42738286]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "21\n",
      "[[ 0.40813935  0.54524112  0.80983048  0.40813935  0.41663408  0.39546454\n",
      "   0.81492288]\n",
      " [ 0.32467046  0.31224899  0.43644963  0.32467046  0.31501457  0.32764416\n",
      "   0.43158361]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "22\n",
      "[[ 0.40969234  0.54690155  0.80606264  0.40969234  0.41853874  0.39695281\n",
      "   0.81131811]\n",
      " [ 0.32371082  0.31157398  0.44059569  0.32371082  0.31386448  0.32669875\n",
      "   0.43563637]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "23\n",
      "[[ 0.41123961  0.54854962  0.80225636  0.41123961  0.42043663  0.3984359\n",
      "   0.80767689]\n",
      " [ 0.32275996  0.31090868  0.4447192   0.32275996  0.31272547  0.32576182\n",
      "   0.43966778]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "24\n",
      "[[ 0.41278102  0.55018523  0.79841248  0.41278102  0.42232757  0.39991369\n",
      "   0.80399999]\n",
      " [ 0.32181783  0.310253    0.44881959  0.32181783  0.31159746  0.32483329\n",
      "   0.44367732]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "25\n",
      "[[ 0.41431646  0.5518083   0.79453189  0.41431646  0.42421138  0.40138605\n",
      "   0.80028821]\n",
      " [ 0.32088436  0.30960685  0.45289632  0.32088436  0.31048033  0.3239131\n",
      "   0.44766447]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "26\n",
      "[[ 0.41584579  0.55341874  0.79061551  0.41584579  0.42608789  0.40285288\n",
      "   0.79654239]\n",
      " [ 0.31995947  0.30897013  0.4569489   0.31995947  0.30937402  0.32300119\n",
      "   0.45162874]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "27\n",
      "[[ 0.41736891  0.55501647  0.78666429  0.41736891  0.42795694  0.40431404\n",
      "   0.7927634 ]\n",
      " [ 0.31904309  0.30834274  0.46097683  0.31904309  0.30827841  0.32209748\n",
      "   0.45556967]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "28\n",
      "[[ 0.41902832  0.55599774  0.78274123  0.41902832  0.42968955  0.40607717\n",
      "   0.78880274]\n",
      " [ 0.31802822  0.30777727  0.46494015  0.31802822  0.3072446   0.32099871\n",
      "   0.45965534]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "29\n",
      "[[ 0.42054274  0.55756688  0.77872395  0.42054274  0.43154312  0.40753341\n",
      "   0.78495467]\n",
      " [ 0.3171262   0.30716612  0.46891789  0.3171262   0.30616964  0.32010751\n",
      "   0.46355203]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "30\n",
      "[[ 0.42219935  0.55853857  0.77473342  0.42219935  0.43326677  0.40929634\n",
      "   0.78092292]\n",
      " [ 0.31612258  0.30660921  0.47283445  0.31612258  0.30515211  0.31901943\n",
      "   0.46759184]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "31\n",
      "[[ 0.42370443  0.56007906  0.77065415  0.42370443  0.43510426  0.41074692\n",
      "   0.77700958]\n",
      " [ 0.31523479  0.30601389  0.47676038  0.31523479  0.30409744  0.31814072\n",
      "   0.47144235]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "32\n",
      "[[ 0.42520269  0.56160651  0.76654523  0.42520269  0.4369335   0.41219124\n",
      "   0.7730677 ]\n",
      " [ 0.31435519  0.30542736  0.48065965  0.31435519  0.303053    0.31726996\n",
      "   0.4752675 ]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "33\n",
      "[[ 0.42685531  0.56255986  0.76246216  0.42685531  0.43864026  0.41395641\n",
      "   0.76893388]\n",
      " [ 0.31336728  0.30488511  0.48450189  0.31336728  0.30206079  0.31619513\n",
      "   0.47923909]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "34\n",
      "[[ 0.42834344  0.56405857  0.7582978   0.42834344  0.44045255  0.41539416\n",
      "   0.76493278]\n",
      " [ 0.31250165  0.30431378  0.48834726  0.31250165  0.30103608  0.31533673\n",
      "   0.4830157 ]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "35\n",
      "[[ 0.42982443  0.5655441   0.75410722  0.42982443  0.44225616  0.41682534\n",
      "   0.76090623]\n",
      " [ 0.31164401  0.30375092  0.49216504  0.31164401  0.30002131  0.31448609\n",
      "   0.48676599]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "36\n",
      "[[ 0.43129818  0.56701643  0.74989159  0.43129818  0.44405096  0.41824984\n",
      "   0.75685534]\n",
      " [ 0.31079429  0.30319642  0.49595497  0.31079429  0.29901637  0.31364314\n",
      "   0.49048972]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "37\n",
      "[[ 0.43294362  0.56794264  0.74570122  0.43294362  0.44573173  0.42001699\n",
      "   0.75259883]\n",
      " [ 0.3098268   0.30267438  0.49969277  0.3098268   0.29805784  0.31258494\n",
      "   0.49436673]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "38\n",
      "[[ 0.43458194  0.56886638  0.74148586  0.43458194  0.44740804  0.42177433\n",
      "   0.74832117]\n",
      " [ 0.3088678   0.30215484  0.50340476  0.3088678   0.2971062   0.31153774\n",
      "   0.49821104]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "39\n",
      "[[ 0.43621307  0.56978735  0.7372468   0.43621307  0.44907951  0.42352195\n",
      "   0.74402328]\n",
      " [ 0.30791721  0.30163793  0.50709057  0.30791721  0.29616154  0.31050129\n",
      "   0.50202285]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "40\n",
      "[[ 0.4376675   0.57120014  0.73294416  0.4376675   0.45083789  0.42493599\n",
      "   0.73987133]\n",
      " [ 0.30709257  0.30110976  0.51076798  0.30709257  0.29519381  0.30967969\n",
      "   0.50564586]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "41\n",
      "[[ 0.43928938  0.57210973  0.72866285  0.43928938  0.45249607  0.42667519\n",
      "   0.73552877]\n",
      " [ 0.3061548   0.30060121  0.51439945  0.3061548   0.29426474  0.3086567\n",
      "   0.50940166]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "42\n",
      "[[ 0.4407315   0.57349403  0.7243239   0.4407315   0.45423556  0.42808\n",
      "   0.73133604]\n",
      " [ 0.30534351  0.30008657  0.51801935  0.30534351  0.29331525  0.30784725\n",
      "   0.51297139]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "43\n",
      "[[ 0.44234317  0.57439199  0.72000537  0.44234317  0.4558798   0.42980959\n",
      "   0.726954  ]\n",
      " [ 0.30441867  0.29958631  0.52159568  0.30441867  0.29240161  0.30683799\n",
      "   0.51667016]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "44\n",
      "[[ 0.44377253  0.57574796  0.71563514  0.44377253  0.4576      0.43120458\n",
      "   0.7227254 ]\n",
      " [ 0.30362055  0.29908479  0.52515757  0.30362055  0.29146999  0.30604061\n",
      "   0.52018583]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "45\n",
      "[[ 0.44519382  0.57709066  0.71125108  0.44519382  0.45931039  0.43259203\n",
      "   0.71848276]\n",
      " [ 0.30282976  0.2985906   0.52869016  0.30282976  0.29054733  0.30525042\n",
      "   0.5236733 ]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "46\n",
      "[[ 0.44660699  0.57842008  0.70685446  0.44660699  0.46101089  0.43397186\n",
      "   0.71422729]\n",
      " [ 0.30204621  0.29810365  0.53219338  0.30204621  0.28963353  0.30446735\n",
      "   0.52713249]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "47\n",
      "[[ 0.44801197  0.57973624  0.70244657  0.44801197  0.4627014   0.43534402\n",
      "   0.7099602 ]\n",
      " [ 0.30126983  0.29762383  0.53566717  0.30126983  0.2887285   0.30369131\n",
      "   0.53056333]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "48\n",
      "[[ 0.44960331  0.58059758  0.69805874  0.44960331  0.46430494  0.43706193\n",
      "   0.70549077]\n",
      " [ 0.30037169  0.29714753  0.53910208  0.30037169  0.28785432  0.30270578\n",
      "   0.53412886]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "49\n",
      "[[ 0.45099471  0.58188579  0.69363228  0.45099471  0.46597531  0.43842332\n",
      "   0.70119989]\n",
      " [ 0.29960788  0.29667983  0.54251719  0.29960788  0.28696621  0.30194133\n",
      "   0.53750463]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "50\n",
      "[[ 0.45257283  0.58273516  0.689225    0.45257283  0.46756317  0.44012761\n",
      "   0.69670975]\n",
      " [ 0.29872302  0.29621135  0.54589544  0.29872302  0.28610664  0.30097045\n",
      "   0.54101028]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "51\n",
      "[[ 0.45414102  0.58358055  0.68480951  0.45414102  0.46914397  0.44181908\n",
      "   0.69221522]\n",
      " [ 0.29784683  0.29574566  0.54924558  0.29784683  0.28525378  0.30001068\n",
      "   0.54448174]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "52\n",
      "[[ 0.45569935  0.5844218   0.68038712  0.45569935  0.47071749  0.44349795\n",
      "   0.68771727]\n",
      " [ 0.29697915  0.29528279  0.55256751  0.29697915  0.28440763  0.29906174\n",
      "   0.54791939]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "53\n",
      "[[ 0.45724783  0.58525878  0.67595916  0.45724783  0.47228353  0.44516443\n",
      "   0.68321689]\n",
      " [ 0.29611983  0.2948228   0.55586119  0.29611983  0.2835682   0.29812334\n",
      "   0.5513236 ]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "54\n",
      "[[ 0.45860741  0.58647663  0.67150831  0.45860741  0.47390251  0.4465027\n",
      "   0.67888344]\n",
      " [ 0.29538472  0.29438106  0.55912993  0.29538472  0.28272041  0.2973846\n",
      "   0.55456256]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "55\n",
      "[[ 0.45995835  0.58768154  0.66705621  0.45995835  0.47551101  0.44783278\n",
      "   0.67454786]\n",
      " [ 0.29465625  0.29394558  0.56236933  0.29465625  0.28188063  0.29665242\n",
      "   0.55777305]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "56\n",
      "[[ 0.46148749  0.58849807  0.66262034  0.46148749  0.47705063  0.4494815\n",
      "   0.67003374]\n",
      " [ 0.29381465  0.29349802  0.56557739  0.29381465  0.2810628   0.29573214\n",
      "   0.5610913 ]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "57\n",
      "[[ 0.46300637  0.58931009  0.65818388  0.46300637  0.47858227  0.45111747\n",
      "   0.66552205]\n",
      " [ 0.29298124  0.29305332  0.56875722  0.29298124  0.28025154  0.29482215\n",
      "   0.5643765 ]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "58\n",
      "[[ 0.46451505  0.5901175   0.65374808  0.46451505  0.48010576  0.45274087\n",
      "   0.66101376]\n",
      " [ 0.29215587  0.29261151  0.57190882  0.29215587  0.27944683  0.29392217\n",
      "   0.56762899]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "59\n",
      "[[ 0.46601356  0.5909202   0.64931414  0.46601356  0.48162098  0.45435187\n",
      "   0.65650985]\n",
      " [ 0.29133842  0.29217261  0.57503224  0.29133842  0.27864864  0.29303199\n",
      "   0.57084912]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "60\n",
      "[[ 0.46732964  0.59205791  0.64487363  0.46732964  0.48317676  0.45565433\n",
      "   0.65216942]\n",
      " [ 0.29063769  0.29176047  0.57812683  0.29063769  0.27784657  0.29232533\n",
      "   0.57392208]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "61\n",
      "[[ 0.46863688  0.59318311  0.64043882  0.46863688  0.48472187  0.45694833\n",
      "   0.64783358]\n",
      " [ 0.28994319  0.29135397  0.58119263  0.28994319  0.27705197  0.2916249\n",
      "   0.57696698]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "62\n",
      "[[ 0.46993526  0.59429587  0.6360108   0.46993526  0.48625628  0.45823386\n",
      "   0.64350338]\n",
      " [ 0.28925488  0.290953    0.58422976  0.28925488  0.27626475  0.2909306\n",
      "   0.57998392]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "63\n",
      "[[ 0.47122479  0.59539624  0.63159065  0.47122479  0.48777999  0.4595109\n",
      "   0.63917985]\n",
      " [ 0.28857266  0.29055747  0.58723835  0.28857266  0.27548481  0.29024239\n",
      "   0.58297303]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "64\n",
      "[[ 0.47269017  0.59616326  0.62718453  0.47269017  0.48924809  0.46109276\n",
      "   0.63469141]\n",
      " [ 0.287782    0.29013897  0.59022123  0.287782    0.27472083  0.28937847\n",
      "   0.58605503]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "65\n",
      "[[ 0.4739639   0.59723851  0.62278323  0.4739639   0.49075042  0.46235586\n",
      "   0.63038189]\n",
      " [ 0.28711083  0.2897526   0.59317331  0.28711083  0.27395501  0.28870085\n",
      "   0.58898958]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "66\n",
      "[[ 0.47541247  0.59799317  0.61839547  0.47541247  0.4922001   0.46391993\n",
      "   0.62591107]\n",
      " [ 0.28633258  0.28934106  0.59610089  0.28633258  0.27320389  0.28785073\n",
      "   0.59201355]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "67\n",
      "[[ 0.47685022  0.59874299  0.61401831  0.47685022  0.49364098  0.46547086\n",
      "   0.62145355]\n",
      " [ 0.28556193  0.28893228  0.59900105  0.28556193  0.27245887  0.28701006\n",
      "   0.59500618]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "68\n",
      "[[ 0.47827722  0.59948791  0.60965275  0.47827722  0.49507298  0.46700883\n",
      "   0.61701011]\n",
      " [ 0.28479876  0.28852626  0.60187394  0.28479876  0.27171992  0.28617859\n",
      "   0.59796785]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "69\n",
      "[[ 0.47952073  0.60051376  0.60530097  0.47952073  0.4965326   0.4682461\n",
      "   0.6127387 ]\n",
      " [ 0.28414825  0.28815582  0.60471493  0.28414825  0.27098116  0.2855205\n",
      "   0.60079528]]\n",
      "rounded [1 1 1 1 1 1 1]\n",
      "70\n",
      "[[ 0.48093061  0.60124642  0.60096161  0.48093061  0.49794585  0.469766\n",
      "   0.60832071]\n",
      " [ 0.28339706  0.28775654  0.60753361  0.28339706  0.27025462  0.2847023\n",
      "   0.60370037]]\n",
      "rounded [1 1 2 1 1 1 1]\n",
      "71\n",
      "[[ 0.4823298   0.60197412  0.59663667  0.4823298   0.49935008  0.47127315\n",
      "   0.60391939]\n",
      " [ 0.28265309  0.28735998  0.61032544  0.28265309  0.269534    0.28389292\n",
      "   0.60657531]]\n",
      "rounded [1 1 2 1 1 1 2]\n",
      "All points classified correctly\n",
      "[0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.8571428571428571, 1.0]\n"
     ]
    }
   ],
   "source": [
    "classifier.train(data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
