{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CS 289A Homework 5 - Data Processing\n",
    "------------------------------------------------\n",
    "This script will load the data sets in need of preprocessing (census and Titanic) and perform the preprocessing for better learning results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with the overhead: load necessary modules and trigger them to reload when they are modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn.feature_extraction import DictVectorizer as DV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, specify paths to data on the local machine. **You must change the path to fit your data.**\n",
    "Note that the census and Titanic data are given as a csv filess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = \"/Users/mitch/Documents/Cal/2_2017_Spring/COMPSCI 289A - Intro to Machine Learning/HW05\"\n",
    "CENS_RAWPATH = \"Data/hw5_census_dist/census_traindata_raw.csv\"\n",
    "CENS_CLNPATH = \"Data/census_traindata.csv\"\n",
    "CENS_LBLPATH = \"Data/census_traindata_lbl.csv\"\n",
    "CENS_VECPATH = \"Data/census_traindata_vec.csv\"\n",
    "TITA_RAWPATH = \"Data/hw5_titanic_dist/titanic_traindata_raw.csv\"\n",
    "TITA_CLNPATH = \"Data/titanic_traindata.csv\"\n",
    "TITA_LBLPATH = \"Data/titanic_traindata_lbl.csv\"\n",
    "TITA_VECPATH = \"Data/titanic_traindata_vec.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute and clean the datasets\n",
    "----------------------------------------\n",
    "First we will impute and clean the census data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "censusdf_raw = pandas.read_csv(open(BASE_DIR+'/'+CENS_PATH))\n",
    "censusdf_nan = censusdf_raw.replace(to_replace='?',value=np.nan)\n",
    "censusdf = censusdf_nan.fillna(censusdf_nan.mode().iloc[0])\n",
    "censusdf.to_csv(BASE_DIR+'/'+CENS_CLNPATH,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the vast majority of the unknown values, denoted with a '?', are categorical rather than continuous datapoints, replace them with the most common category-value in that feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, repeat the imputation and cleaning for the Titanic dataset: (recognizing that the cabin feature vector is incredibly sparse--and presumably meaningless--eliminate it from the data set to be processed; similarly, due to the large variation in data types in the ticket column, remove it as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanicdf_raw = pandas.read_csv(open(BASE_DIR+'/'+TITA_PATH))\n",
    "titanicdf_nan = titanicdf_raw.replace(to_replace='',value=np.nan)\n",
    "titanicdf = titanicdf_nan.fillna(titanicdf_nan.mode().iloc[0])\n",
    "titanicdf.drop('cabin',axis=1,inplace=True)\n",
    "titanicdf.drop('ticket',axis=1,inplace=True)\n",
    "titanicdf.to_csv(BASE_DIR+'/'+TITA_CLNPATH,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the cleaned and full data\n",
    "--------------------------------------\n",
    "First, separate the labels from the data, and save to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "censuslbldf = censusdf['label']\n",
    "censuslbldf.to_csv(BASE_DIR+'/'+CENS_LBLPATH,index=False)\n",
    "censusdf.drop('label',axis=1,inplace=True)\n",
    "\n",
    "titaniclbldf = titanicdf['survived']\n",
    "titaniclbldf.to_csv(BASE_DIR+'/'+TITA_LBLPATH,index=False)\n",
    "titanicdf.drop('survived',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the DictVectorizer class from sklearn to create vectors for categorical mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For the census dataset\n",
    "censusdict = censusdf.to_dict('records')\n",
    "dv = DV(sparse=False)\n",
    "censusvec = dv.fit_transform(censusdict)\n",
    "np.savetxt(BASE_DIR+'/'+CENS_VECPATH,censusvec,fmt='%10d',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For the Titanic dataset\n",
    "titanicdict = titanicdf.to_dict('records')\n",
    "dv = DV(sparse=False)\n",
    "titanicvec = dv.fit_transform(titanicdict)\n",
    "np.savetxt(BASE_DIR+'/'+TITA_VECPATH,titanicvec,fmt='%10d',delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
