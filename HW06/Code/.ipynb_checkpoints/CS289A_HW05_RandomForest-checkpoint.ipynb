{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Constructor\n",
    "\n",
    "This notebook adapts the decision tree class to a random forest to solve problem 2. (It will eventually be imported into its own python module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class RandomDecisionTree:\n",
    "    \"\"\"\n",
    "    Build and store a random decision tree, based on supplied training data. \n",
    "    Use this tree to predict classifications.\n",
    "    - treedepth: an integer for the max depth of the tree\n",
    "    - mfeatures: an integer number of random features tested for splits per node\n",
    "    - verbose:   a boolean for descriptive output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,treedepth=10,mfeatures=None,verbose=False):\n",
    "        self.depth = treedepth\n",
    "        self.mfeatures = mfeatures\n",
    "        self.nfeatures = None\n",
    "        self.verbose = verbose\n",
    "        self.tree = self.Node()\n",
    "        if type(treedepth) is not int:\n",
    "            print('ERROR (RandomDecisionTree): Tree depth must be an integer.')\n",
    "        if mfeatures and type(mfeatures) is not int:\n",
    "            print('ERROR (RandomDecisionTree): The number of random features must be an integer.')\n",
    "        \n",
    "        \n",
    "    def entropy(self,C,D,c,d):\n",
    "        \"\"\"\n",
    "        Calculate entropy based on classifications above and below the \n",
    "        splitrule.\n",
    "        - C: sample points in-class below splitrule (left)\n",
    "        - D: sample points not-in-class below splitrule (left)\n",
    "        - c: sample points in-class above splitrule (right)\n",
    "        - d: sample points not-in-class above splitrule (right)\n",
    "        Returns the entropy.\n",
    "        \"\"\"\n",
    "        \n",
    "        if C != 0:\n",
    "            Cfactor = -(C/(C+D))*np.log2(C/(C+D))\n",
    "        else:\n",
    "            Cfactor = 0\n",
    "        if D != 0:\n",
    "            Dfactor = -(D/(C+D))*np.log2(D/(C+D))\n",
    "        else:\n",
    "            Dfactor = 0\n",
    "        if c != 0:\n",
    "            cfactor = -(c/(c+d))*np.log2(c/(c+d))\n",
    "        else:\n",
    "            cfactor = 0\n",
    "        if d != 0:\n",
    "            dfactor = -(d/(c+d))*np.log2(d/(c+d))\n",
    "        else:\n",
    "            dfactor = 0\n",
    "        H_left = Cfactor + Dfactor\n",
    "        H_right = cfactor + dfactor\n",
    "        H = ((C+D)*H_left + (c+d)*H_right)/(C+D+c+d)\n",
    "        \n",
    "        return H\n",
    "    \n",
    "    \n",
    "    def pick_random_features(self):\n",
    "        \"\"\"Randomly choose a set of m features out of n total features\"\"\"\n",
    "        \n",
    "        mrandomfeatures = -1*np.ones(self.mfeatures)\n",
    "        for i in range(self.mfeatures):\n",
    "            while mrandomfeatures[i] == -1:\n",
    "                feature_i = np.random.randint(self.nfeatures)\n",
    "                if feature_i not in mrandomfeatures:\n",
    "                    mrandomfeatures[i] = feature_i\n",
    "        mrandomfeatures = np.sort(mrandomfeatures).astype(int)\n",
    "        \n",
    "        return mrandomfeatures \n",
    "    \n",
    "    \n",
    "    def segment(self,data,labels):\n",
    "        \"\"\"\n",
    "        March through data and determine split which maximizes info gain.\n",
    "        Returns the ideal splitrule as a length-2 list where the first element\n",
    "        is the index of the splitting feature and the second element is the \n",
    "        value of that feature to split on.\n",
    "        \"\"\"\n",
    "        \n",
    "        totals = np.bincount(labels)\n",
    "        if len(totals)==1:\n",
    "            totals = np.append(totals,[0])\n",
    "        # Quick safety check\n",
    "        if len(labels) != len(data):\n",
    "            print('ERROR (RandomForest.segment): There must be the same number of labels as datapoints.')\n",
    "        \n",
    "        # Calculate the initial entropy, used to find info gain\n",
    "        C,D = 0,0                      # C = in class left of split; D = not in class left of split\n",
    "        c,d = totals[1],totals[0]      # c = in class right of split; d = not in class right of split\n",
    "        H_i = self.entropy(C,D,c,d) # the initial entropy, before any splitting\n",
    "        \n",
    "        # Initialize objects to store optimal split rules for iterative comparison\n",
    "        maxinfogain = 0\n",
    "        splitrule = []   \n",
    "        \n",
    "        mrandomfeatures = self.pick_random_features()\n",
    "        for feature_i in mrandomfeatures:\n",
    "            # Order the data for determining ideal splits\n",
    "            lbldat = np.concatenate(([data[:,feature_i]],[labels]),axis=0)\n",
    "            \n",
    "            fv = np.sort(lbldat.T,axis=0)\n",
    "            lastfeature = np.array(['',''])\n",
    "            \n",
    "            C,D = 0,0                      # Reset the counters\n",
    "            c,d = totals[1],totals[0]\n",
    "            \n",
    "            for point_i in range(len(fv)-1):\n",
    "                \n",
    "                # Update C,D,c,d to minmize runtime of entrop calc (keep at O(1) time)\n",
    "                if fv[point_i,1] == 1:\n",
    "                    C += 1\n",
    "                    c -= 1\n",
    "                elif fv[point_i,1] == 0:\n",
    "                    D += 1\n",
    "                    d -= 1\n",
    "                else:\n",
    "                    print(\"ERROR (RandomForest.segment): Classifications can only be 0 or 1.\")\n",
    "                \n",
    "                # Skip splitting values that are not separable\n",
    "                if fv[point_i,0] == fv[point_i+1,0]:\n",
    "                    continue\n",
    "                else:\n",
    "                    H_f = self.entropy(C,D,c,d)\n",
    "                    infogain = H_i-H_f\n",
    "                    if infogain > maxinfogain:\n",
    "                        maxinfogain = infogain\n",
    "                        splitrule = [feature_i,fv[point_i,0]]\n",
    "        \n",
    "        return splitrule\n",
    "        \n",
    "        \n",
    "    def train(self,data,labels,node=1,deep=0):\n",
    "        \"\"\"\n",
    "        Train the random decision tree on input data\n",
    "        - data:   Nxd numppy array with N sample points and d features\n",
    "        - labels: 1D, length-N numpy array with labels for the N sample points\n",
    "        - node:   node class passed to function; default is 1, a flag for the head node (INTERNAL USE ONLY)\n",
    "        - deep:   a counter to determine current depth in tree (INTERNAL USE ONLY)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Ensure labels are integers\n",
    "        labels = labels.astype(int)\n",
    "\n",
    "        # On the first training cycle, set the current node to the head node\n",
    "        # If the number of random features has not yet been set, set that too.\n",
    "        if node==1:\n",
    "            node=self.tree\n",
    "            if self.mfeatures is None:\n",
    "                self.mfeatures = np.int(np.round((np.sqrt(len(data[0])))))  # m random features\n",
    "            self.nfeatures = len(data[0])                    # n total features\n",
    "            if self.mfeatures > self.nfeatures:\n",
    "                print('WARNING: The number of random features to choose is greater than the total number of features. Using all of the features instead')\n",
    "                self.mfeatures = self.nfeatures\n",
    "        # Grow decision tree\n",
    "        depthlim = self.depth\n",
    "        if deep < depthlim:\n",
    "            splitrule = self.segment(data,labels)\n",
    "        else:\n",
    "            splitrule = []\n",
    "        if self.verbose is True:\n",
    "            print(data,labels)\n",
    "        node.isleaf(data,labels,splitrule)\n",
    "        \n",
    "        # Train deeper if the node splits\n",
    "        if node.nodetype == 'SplitNode':\n",
    "            if self.verbose is True:\n",
    "                print('rule:',node.rule)\n",
    "                print('Splitting node left and right')\n",
    "            deep += 1\n",
    "            node.left=self.Node()\n",
    "            node.right=self.Node()\n",
    "            self.train(node.leftdata,node.leftlabels,node.left,deep)\n",
    "            self.train(node.rightdata,node.rightlabels,node.right,deep)\n",
    "        elif node.nodetype == 'LeafNode':\n",
    "            if self.verbose is True:\n",
    "                print('You made a leaf node! It has value',node.leaflabel,'and',node.leafcount,'items.')            \n",
    "        else:\n",
    "            print('ERROR (RandomForest.train): The node type could not be identified!')\n",
    "    \n",
    "        \n",
    "    def predict(self,testdata):\n",
    "        \"\"\"\n",
    "        Predict classfications for unlabeled data points using the previously \n",
    "        trained random decision tree.\n",
    "        - testdata: Nxd numpy array with N sample points and d features\n",
    "                    *Note, dimensions N and d must match those used \n",
    "                    for data array in DecisionTree.train*\n",
    "        Returns a 1D, length-N numpy array of predictions (one prediction per point)\n",
    "        \"\"\"\n",
    "        \n",
    "        npoints = len(testdata)\n",
    "        predictions = np.empty(npoints)\n",
    "        for point_i in range(npoints):\n",
    "            ParentNode = self.tree\n",
    "            Rule = ParentNode.rule\n",
    "            while Rule is not None:\n",
    "                splitfeat_i = Rule[0]\n",
    "                splitval = Rule[1]\n",
    "                if testdata[point_i,splitfeat_i] <= splitval:\n",
    "                    ChildNode = ParentNode.left\n",
    "                else:\n",
    "                    ChildNode = ParentNode.right\n",
    "                ParentNode = ChildNode\n",
    "                Rule = ParentNode.rule\n",
    "            predictions[point_i]=ParentNode.leaflabel\n",
    "        \n",
    "        return predictions.astype(int)\n",
    "    \n",
    "    \n",
    "    \n",
    "    class Node:\n",
    "        \"\"\"\n",
    "        Store a decision tree node, coupled in series to construct tree;\n",
    "        includes a left branch, right branch, and splitrule\n",
    "        \"\"\"\n",
    "    \n",
    "        def __init__(self):\n",
    "            self.rule = None\n",
    "            self.left = None\n",
    "            self.leftdata = None\n",
    "            self.leftlabels = None\n",
    "            self.right = None\n",
    "            self.rightdata = None\n",
    "            self.rightlabels = None\n",
    "            self.leaflabel = None\n",
    "            self.leafcount = None\n",
    "            self.nodetype = None\n",
    "            \n",
    "            \n",
    "        def isleaf(self,data,labels,splitrule):\n",
    "            \"\"\"Determine if this is a leaf node\"\"\"\n",
    "            \n",
    "            if splitrule:\n",
    "                indsabove = self.datainds_above_split(data,splitrule)\n",
    "                self.rule = splitrule\n",
    "                self.leftdata,self.leftlabels = self.leftDL(data,labels,indsabove)\n",
    "                self.rightdata,self.rightlabels = self.rightDL(data,labels,indsabove)\n",
    "                self.nodetype = 'SplitNode'\n",
    "            \n",
    "            elif not splitrule:\n",
    "                self.leaflabel = np.bincount(labels).argmax()\n",
    "                self.leafcount = len(labels)\n",
    "                self.nodetype = 'LeafNode'\n",
    "\n",
    "         \n",
    "        def datainds_above_split(self,data,splitrule):\n",
    "            \"\"\"\n",
    "            Collect indices of points with values of the splitting feature\n",
    "            greater than the split rule\n",
    "            \"\"\"\n",
    "            \n",
    "            indsabove = []\n",
    "            fv = data[:,splitrule[0]]\n",
    "            for point_i in range(len(fv)):\n",
    "                if fv[point_i] > splitrule[1]:\n",
    "                    indsabove.append(point_i)\n",
    "            \n",
    "            return indsabove\n",
    "              \n",
    "        \n",
    "        def leftDL(self,data,labels,indsabove):\n",
    "            \"\"\"Return arrays of only left data and labels\"\"\"\n",
    "            \n",
    "            leftdata = np.delete(data,indsabove,axis=0)\n",
    "            leftlabels = np.delete(labels,indsabove,axis=0)\n",
    "            \n",
    "            return leftdata,leftlabels   \n",
    "        \n",
    "        \n",
    "        def rightDL(self,data,labels,indsabove):\n",
    "            \"\"\"Return arrays of only right data and labels\"\"\"\n",
    "            \n",
    "            rightdata = data[indsabove]\n",
    "            rightlabels = labels[indsabove]\n",
    "            \n",
    "            return rightdata,rightlabels\n",
    "        \n",
    "\n",
    "class RandomForest:\n",
    "    \"\"\"\n",
    "    Build and store a random forest, based on supplied training data. \n",
    "    Use this tree to predict classifications.\n",
    "    - treedepth: an integer for the max depth of any tree in the forest\n",
    "    - mfeatures: an integer number of random features tested for splits per node\n",
    "    - verbose:   a boolean for descriptive output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,treedepth=10,ntrees=None,mfeatures=None,verbose=False):\n",
    "        self.treedepth = treedepth\n",
    "        self.mfeatures = mfeatures\n",
    "        self.verbose = verbose\n",
    "        self.treecount = ntrees\n",
    "        self.forest = []\n",
    "        if type(treedepth) is not int:\n",
    "            print('ERROR (RandomForest): Tree depth must be an integer.')\n",
    "        if mfeatures and type(mfeatures) is not int:\n",
    "            print('ERROR (RandomForest): The number of random features must be an integer.')\n",
    "            \n",
    "            \n",
    "    def train(self,data,labels):\n",
    "        \"\"\"\n",
    "        Train (grow) the random forest on input data\n",
    "        - data:   Nxd numppy array with N sample points and d features\n",
    "        - labels: 1D, length-N numpy array with labels for the N sample points\n",
    "        \"\"\"\n",
    "        if self.treecount is None:\n",
    "            self.treecount = int(np.sqrt(len(data)))\n",
    "        elif type(self.treecount) is not int:\n",
    "            print('ERROR (RandomForest): The number of trees must be an integer.')\n",
    "            \n",
    "        for tree_i in range(self.treecount):\n",
    "            tree = RandomDecisionTree(self.treedepth,self.mfeatures,self.verbose)\n",
    "            tree.train(data,labels)\n",
    "            self.forest.append(tree)\n",
    "            if tree_i%5 == 0:\n",
    "                print('Finished training %i tree(s) out of %i' %(tree_i,self.treecount))\n",
    "            \n",
    "    def predict(self,testdata):\n",
    "        \"\"\"\n",
    "        Predict classfications for unlabeled data points using the previously \n",
    "        trained random forest.\n",
    "        - testdata: Nxd numpy array with N sample points and d features\n",
    "                    *Note, dimensions N and d must match those used \n",
    "                    for data array in DecisionTree.train*\n",
    "        Returns a 1D, length-N numpy array of predictions (one prediction per point)\n",
    "        \"\"\"\n",
    "        \n",
    "        aggregatedpredictions = np.empty((self.treecount,len(testdata)))\n",
    "        for tree_i in range(self.treecount):\n",
    "            treepredictions = self.forest[tree_i].predict(testdata)\n",
    "            aggregatedpredictions[tree_i]=treepredictions\n",
    "        forestpredictions = np.round(np.average(aggregatedpredictions,axis=0)).astype(int)\n",
    "        \n",
    "        return forestpredictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = RandomForest(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.array([[2,2,2,5],[1,1,1,2],[10,3,1,6],[2,2,2,5],[2,1,5,3],[2,2,2,6],[8,1,2,1]])\n",
    "labels = np.array([1,1,0,1,0,1,1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.train(data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([[10,3,1,5],[2,1,2,4],[2,2,2,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predict(x).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  3  1  5]\n",
      " [ 2  1  2  4]\n",
      " [ 2  2  2  4]]\n",
      "[ 4.66666667  2.          1.66666667  4.33333333]\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(np.average(x,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
