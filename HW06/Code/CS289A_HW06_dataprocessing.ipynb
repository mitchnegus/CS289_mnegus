{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CS 289A Homework 6 - Data Processing\n",
    "------------------------------------------------\n",
    "This script will load the letters data set, then perform preprocessing (shuffling, separation into training and validation sets, and normalization). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with the overhead: load necessary modules and trigger them to reload when they are modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import HW06_utils as ut\n",
    "import numpy as np\n",
    "from scipy import io as spio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, specify paths to data on the local machine. \n",
    "**You must change the path to fit your data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = \"/Users/mitch/Documents/Cal/2_2017_Spring/COMPSCI 289A - Intro to Machine Learning/HW06/\"\n",
    "DATA_PATH = \"Data/hw6_data_dist/letters_data.mat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_raw = ut.load_data(DATA_PATH,BASE_DIR,'train_x')\n",
    "y_train_raw = ut.load_data(DATA_PATH,BASE_DIR,'train_y')\n",
    "X_test_raw = ut.load_data(DATA_PATH,BASE_DIR,'test_x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Center and normalize the data by  \n",
    "(1) contrast-normalizing the images (dividing by $\\ell_2$ norm of the pixel values)  \n",
    "(2) subtracting the mean of the training data from both training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the l_2 norm of pixel values for each image\n",
    "image2norms_train = np.array([np.linalg.norm(X_train_raw,axis=1)]).T\n",
    "image2norms_test = np.array([np.linalg.norm(X_test_raw,axis=1)]).T\n",
    "# Divide each image pixel values by its l_2 norm\n",
    "X_train_norm = X_train_raw/image2norms_train\n",
    "X_test_norm = X_test_raw/image2norms_test\n",
    "\n",
    "# Generate a matrix where each row corresponds to the mean of the dataset\n",
    "mu_i = np.mean(X_train_norm,axis=0)\n",
    "mu_train = np.array([mu_i for i in X_train_norm])\n",
    "mu_test = np.array([mu_i for i in X_test_norm])\n",
    "# Subtract the mean from the data\n",
    "X_train_center = X_train_norm-mu_train\n",
    "X_test_center = X_test_norm-mu_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle training data and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_shuf,y_train_shuf = ut.shuffle_data(X_train_center,y_train_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate a validation set of 20% of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train,X_val = ut.val_partition(X_train_shuf,0.20)\n",
    "y_train,y_val = ut.val_partition(y_train_shuf,0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save training data/labels, validation data/labels, and test data/labels to CSV files for future access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(BASE_DIR+\"Data/letters_traindata.csv\",X_train,fmt='%.4e', delimiter=',')\n",
    "np.savetxt(BASE_DIR+\"Data/letters_trainlabels.csv\",y_train,fmt='%i', delimiter=',')\n",
    "np.savetxt(BASE_DIR+\"Data/letters_valdata.csv\",X_val,fmt='%.4e', delimiter=',')\n",
    "np.savetxt(BASE_DIR+\"Data/letters_vallabels.csv\",y_val,fmt='%i', delimiter=',')\n",
    "np.savetxt(BASE_DIR+\"Data/letters_testdata.csv\",X_test_center,fmt='%.4e', delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
